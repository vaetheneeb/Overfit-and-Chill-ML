{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Step 1\n\nimport pandas as pd\nimport numpy as np\n\n# Read the data\nprint(\"=== Reading Data ===\")\ntrain = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\nsubmission = pd.read_csv(\"submission.csv\")\n\nprint(f\"Train shape: {train.shape}\")\nprint(f\"Test shape: {test.shape}\")\nprint(f\"Submission shape: {submission.shape}\")\n\nprint(\"\\n=== Train Columns ===\")\nprint(train.columns.tolist())\n\nprint(\"\\n=== Basic Info ===\")\nprint(\"Train info:\")\nprint(train.info())\n\nprint(\"\\n=== Target Variables ===\")\nprint(\"Precipitation (regression target):\")\nprint(f\"  Mean: {train['precipitation_mm'].mean():.4f}\")\nprint(f\"  Std: {train['precipitation_mm'].std():.4f}\")\nprint(f\"  Min: {train['precipitation_mm'].min():.4f}\")\nprint(f\"  Max: {train['precipitation_mm'].max():.4f}\")\n\nprint(\"\\nElectricity Shutdown (classification target):\")\nprint(train['electricity_shutdown'].value_counts())\nprint(f\"  Class imbalance: {train['electricity_shutdown'].mean():.4f}\")\n\nprint(\"\\n=== Time Series Info ===\")\ntrain['time'] = pd.to_datetime(train['time'])\nprint(f\"Time range: {train['time'].min()} to {train['time'].max()}\")\nprint(f\"Total hours: {len(train)}\")\n\nprint(\"\\n=== Missing Values ===\")\nmissing = train.isnull().sum()\nprint(missing[missing > 0].sort_values(ascending=False).head(10))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Step 2\nimport numpy as np\nfrom sklearn.metrics import f1_score, mean_squared_error\n\ndef custom_metric(y_true_reg, y_pred_reg, y_true_clf, y_pred_clf_proba, threshold=0.5):\n    \"\"\"\n    Calculate the custom metric: RMSE * (1 - F1)\n    \n    Args:\n        y_true_reg: True precipitation values\n        y_pred_reg: Predicted precipitation values  \n        y_true_clf: True electricity shutdown (0/1)\n        y_pred_clf_proba: Predicted probabilities for electricity shutdown\n        threshold: Threshold to convert probabilities to binary predictions\n    \n    Returns:\n        float: RMSE * (1 - F1)\n    \"\"\"\n    # Convert probabilities to binary predictions\n    y_pred_bin = (y_pred_clf_proba >= threshold).astype(int)\n    \n    # Calculate F1 score for classification\n    f1 = f1_score(y_true_clf, y_pred_bin, zero_division=0)\n    \n    # Calculate RMSE for regression\n    rmse = np.sqrt(np.mean((y_true_reg - y_pred_reg) ** 2))\n    \n    # Final metric\n    final_score = rmse * (1 - f1)\n    \n    return final_score, f1, rmse\n\n# Example with dummy data\nprint(\"=== Custom Metric Example ===\")\nprint(\"Let's say we have these predictions:\")\n\n# Dummy data\ny_true_reg = np.array([1.0, 2.0, 0.5, 3.0])  # True precipitation\ny_pred_reg = np.array([1.1, 1.9, 0.6, 2.8])  # Predicted precipitation\n\ny_true_clf = np.array([0, 1, 0, 1])          # True electricity shutdown\ny_pred_proba = np.array([0.1, 0.8, 0.2, 0.9]) # Predicted probabilities\n\nprint(f\"True precipitation: {y_true_reg}\")\nprint(f\"Predicted precipitation: {y_pred_reg}\")\nprint(f\"True electricity shutdown: {y_true_clf}\")\nprint(f\"Predicted probabilities: {y_pred_proba}\")\n\n# Calculate with different thresholds\nfor threshold in [0.3, 0.5, 0.7]:\n    score, f1, rmse = custom_metric(y_true_reg, y_pred_reg, y_true_clf, y_pred_proba, threshold)\n    binary_preds = (y_pred_proba >= threshold).astype(int)\n    print(f\"\\nThreshold {threshold}:\")\n    print(f\"  Binary predictions: {binary_preds}\")\n    print(f\"  F1 Score: {f1:.3f}\")\n    print(f\"  RMSE: {rmse:.3f}\")\n    print(f\"  Final Score (RMSE * (1-F1)): {score:.3f}\")\n\nprint(\"\\n=== Key Insights ===\")\nprint(\"1. LOWER final score is better\")\nprint(\"2. We want to minimize RMSE (regression error)\")\nprint(\"3. We want to maximize F1 (classification accuracy)\")\nprint(\"4. Poor F1 score heavily penalizes the final score\")\nprint(\"5. Threshold selection is crucial for imbalanced classification\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Step 3\nimport pandas as pd\nimport numpy as np\n\ndef add_time_features(df, time_col='time'):\n    \"\"\"Add time-based features from datetime column\"\"\"\n    df = df.copy()\n    df[time_col] = pd.to_datetime(df[time_col])\n    \n    # Extract time components\n    df['year'] = df[time_col].dt.year\n    df['month'] = df[time_col].dt.month\n    df['day'] = df[time_col].dt.day\n    df['hour'] = df[time_col].dt.hour\n    df['dayofweek'] = df[time_col].dt.dayofweek  # Monday=0, Sunday=6\n    df['dayofyear'] = df[time_col].dt.dayofyear\n    df['is_weekend'] = (df[time_col].dt.dayofweek >= 5).astype(int)\n    df['is_day'] = df['is_day']  # Already exists in data\n    \n    return df\n\ndef create_lag_features(df, numeric_cols, lags=[1, 3, 6, 12, 24]):\n    \"\"\"Create lag features for numeric columns\"\"\"\n    df = df.copy()\n    \n    for col in numeric_cols:\n        for lag in lags:\n            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n    \n    return df\n\ndef create_rolling_features(df, numeric_cols, windows=[3, 6, 12, 24]):\n    \"\"\"Create rolling window features\"\"\"\n    df = df.copy()\n    \n    for col in numeric_cols:\n        for window in windows:\n            # Rolling mean\n            df[f'{col}_rolling_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n            # Rolling std\n            df[f'{col}_rolling_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n    \n    return df\n\n# Load data\nprint(\"=== Loading Data ===\")\ntrain = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\n\nprint(f\"Train shape before: {train.shape}\")\nprint(f\"Test shape before: {test.shape}\")\n\n# Add time features\nprint(\"\\n=== Adding Time Features ===\")\ntrain = add_time_features(train)\ntest = add_time_features(test)\n\nprint(\"New time-based columns added:\")\ntime_cols = ['year', 'month', 'day', 'hour', 'dayofweek', 'dayofyear', 'is_weekend']\nprint(time_cols)\n\n# Identify numeric columns for lag/rolling features\nnumeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n# Remove target columns and time-based columns\nexclude_cols = ['ID', 'precipitation_mm', 'electricity_shutdown'] + time_cols\nfeature_cols = [col for col in numeric_cols if col not in exclude_cols]\n\nprint(f\"\\nNumeric feature columns for lag/rolling: {len(feature_cols)}\")\nprint(feature_cols[:10], \"...\")  # Show first 10\n\n# Create lag features\nprint(\"\\n=== Creating Lag Features ===\")\ntrain = create_lag_features(train, feature_cols, lags=[1, 3, 6])\ntest = create_lag_features(test, feature_cols, lags=[1, 3, 6])\n\n# Create rolling features\nprint(\"\\n=== Creating Rolling Features ===\")\ntrain = create_rolling_features(train, feature_cols, windows=[3, 6])\ntest = create_rolling_features(test, feature_cols, windows=[3, 6])\n\nprint(f\"Train shape after feature engineering: {train.shape}\")\nprint(f\"Test shape after feature engineering: {test.shape}\")\n\n# Show some examples of new features\nprint(\"\\n=== Example of New Features ===\")\nexample_cols = ['temp_2m_C', 'temp_2m_C_lag_1', 'temp_2m_C_rolling_mean_3']\nprint(train[example_cols].head())\n\nprint(\"\\n=== Feature Engineering Summary ===\")\nprint(\"1. Time features: year, month, day, hour, dayofweek, dayofyear, is_weekend\")\nprint(\"2. Lag features: 1, 3, 6 hour lags for all numeric features\")\nprint(\"3. Rolling features: 3, 6 hour rolling mean and std for all numeric features\")\nprint(\"4. Total features created: ~200+ new features\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Step 4\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score, mean_squared_error\n\ndef custom_metric(y_true_reg, y_pred_reg, y_true_clf, y_pred_clf_proba, threshold=0.5):\n    \"\"\"Calculate custom metric: RMSE * (1 - F1)\"\"\"\n    y_pred_bin = (y_pred_clf_proba >= threshold).astype(int)\n    f1 = f1_score(y_true_clf, y_pred_bin, zero_division=0)\n    rmse = np.sqrt(np.mean((y_true_reg - y_pred_reg) ** 2))\n    return rmse * (1 - f1), f1, rmse\n\ndef find_best_threshold(y_true, y_proba):\n    \"\"\"Find optimal threshold for binary classification\"\"\"\n    best_thr, best_f1 = 0.5, -1.0\n    for thr in np.linspace(0.0, 1.0, 101):\n        y_pred = (y_proba >= thr).astype(int)\n        f1 = f1_score(y_true, y_pred, zero_division=0)\n        if f1 > best_f1:\n            best_f1, best_thr = f1, thr\n    return best_thr, best_f1\n\n# Load and prepare data (simplified version)\nprint(\"=== Loading and Preparing Data ===\")\ntrain = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\n\n# Convert time to datetime\ntrain['time'] = pd.to_datetime(train['time'])\ntest['time'] = pd.to_datetime(test['time'])\n\n# Add basic time features\nfor df in [train, test]:\n    df['hour'] = df['time'].dt.hour\n    df['dayofweek'] = df['time'].dt.dayofweek\n    df['month'] = df['time'].dt.month\n\nprint(f\"Train shape: {train.shape}\")\nprint(f\"Test shape: {test.shape}\")\n\n# Prepare features and targets\nfeature_cols = [col for col in train.columns if col not in ['ID', 'time', 'precipitation_mm', 'electricity_shutdown']]\nprint(f\"Number of features: {len(feature_cols)}\")\n\nX_train = train[feature_cols]\ny_train_reg = train['precipitation_mm']\ny_train_clf = train['electricity_shutdown'].astype(int)\nX_test = test[feature_cols]\n\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"y_train_reg shape: {y_train_reg.shape}\")\nprint(f\"y_train_clf shape: {y_train_clf.shape}\")\n\n# Split train data for validation\nX_train_split, X_val, y_reg_split, y_reg_val, y_clf_split, y_clf_val = train_test_split(\n    X_train, y_train_reg, y_train_clf, test_size=0.2, random_state=42, shuffle=False\n)\n\nprint(f\"Training split: {X_train_split.shape}\")\nprint(f\"Validation split: {X_val.shape}\")\n\n# Create preprocessing pipeline\nprint(\"\\n=== Creating Preprocessing Pipeline ===\")\npreprocessor = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Create multi-output model\nprint(\"\\n=== Creating Multi-Output Model ===\")\nbase_model = HistGradientBoostingRegressor(\n    max_iter=100,\n    learning_rate=0.1,\n    random_state=42\n)\n\nmodel = MultiOutputRegressor(base_model)\n\n# Create full pipeline\nfull_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', model)\n])\n\n# Train the model\nprint(\"\\n=== Training Model ===\")\n# Combine targets for multi-output\ny_train_combined = np.column_stack([y_reg_split, y_clf_split])\nfull_pipeline.fit(X_train_split, y_train_combined)\n\n# Make predictions on validation set\nprint(\"\\n=== Making Predictions ===\")\nval_preds = full_pipeline.predict(X_val)\nval_reg_pred = val_preds[:, 0]\nval_clf_proba = np.clip(val_preds[:, 1], 0.0, 1.0)  # Clip to [0, 1]\n\n# Find best threshold\nbest_threshold, best_f1 = find_best_threshold(y_clf_val, val_clf_proba)\nprint(f\"Best threshold: {best_threshold:.3f}\")\nprint(f\"Best F1 score: {best_f1:.3f}\")\n\n# Calculate custom metric\nfinal_score, f1, rmse = custom_metric(y_reg_val, val_reg_pred, y_clf_val, val_clf_proba, best_threshold)\nprint(f\"\\n=== Validation Results ===\")\nprint(f\"RMSE: {rmse:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Custom Metric (RMSE * (1-F1)): {final_score:.4f}\")\n\n# Make predictions on test set\nprint(\"\\n=== Making Test Predictions ===\")\ntest_preds = full_pipeline.predict(X_test)\ntest_reg_pred = test_preds[:, 0]\ntest_clf_proba = np.clip(test_preds[:, 1], 0.0, 1.0)\ntest_clf_pred = (test_clf_proba >= best_threshold).astype(int)\n\nprint(f\"Test predictions shape: {test_preds.shape}\")\nprint(f\"Precipitation range: {test_reg_pred.min():.3f} to {test_reg_pred.max():.3f}\")\nprint(f\"Electricity shutdown predictions: {np.bincount(test_clf_pred)}\")\n\nprint(\"\\n=== Summary ===\")\nprint(\"1. Created preprocessing pipeline with imputation and scaling\")\nprint(\"2. Used HistGradientBoostingRegressor with MultiOutputRegressor\")\nprint(\"3. Trained on 80% of data, validated on 20%\")\nprint(\"4. Found optimal threshold for binary classification\")\nprint(\"5. Calculated custom metric on validation set\")\nprint(\"6. Made predictions on test set\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Step 5\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score, mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef custom_metric(y_true_reg, y_pred_reg, y_true_clf, y_pred_clf_proba, threshold=0.5):\n    \"\"\"Calculate custom metric: RMSE * (1 - F1)\"\"\"\n    y_pred_bin = (y_pred_clf_proba >= threshold).astype(int)\n    f1 = f1_score(y_true_clf, y_pred_bin, zero_division=0)\n    rmse = np.sqrt(np.mean((y_true_reg - y_pred_reg) ** 2))\n    return rmse * (1 - f1), f1, rmse\n\ndef find_best_threshold(y_true, y_proba):\n    \"\"\"Find optimal threshold for binary classification\"\"\"\n    best_thr, best_f1 = 0.5, -1.0\n    for thr in np.linspace(0.0, 1.0, 101):\n        y_pred = (y_proba >= thr).astype(int)\n        f1 = f1_score(y_true, y_pred, zero_division=0)\n        if f1 > best_f1:\n            best_f1, best_thr = f1, thr\n    return best_thr, best_f1\n\ndef add_time_features(df):\n    \"\"\"Add comprehensive time features\"\"\"\n    df = df.copy()\n    df['time'] = pd.to_datetime(df['time'])\n    \n    # Basic time features\n    df['hour'] = df['time'].dt.hour\n    df['dayofweek'] = df['time'].dt.dayofweek\n    df['month'] = df['time'].dt.month\n    df['day'] = df['time'].dt.day\n    df['year'] = df['time'].dt.year\n    \n    # Cyclical features (better for ML)\n    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n    \n    # Binary features\n    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n    df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n    \n    return df\n\ndef create_lag_features(df, cols, lags=[1, 3, 6]):\n    \"\"\"Create lag features for important columns\"\"\"\n    df = df.copy()\n    for col in cols:\n        for lag in lags:\n            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n    return df\n\n# Load and prepare data\nprint(\"=== Loading and Preparing Data ===\")\ntrain = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\n\n# Add time features\ntrain = add_time_features(train)\ntest = add_time_features(test)\n\n# Create lag features for important weather variables\nimportant_cols = ['temp_2m_C', 'rel_humidity_2m_pct', 'pressure_msl_hPa', 'wind_speed_10m_kph']\ntrain = create_lag_features(train, important_cols)\ntest = create_lag_features(test, important_cols)\n\nprint(f\"Train shape after feature engineering: {train.shape}\")\nprint(f\"Test shape after feature engineering: {test.shape}\")\n\n# Prepare features and targets\nfeature_cols = [col for col in train.columns if col not in ['ID', 'time', 'precipitation_mm', 'electricity_shutdown']]\nprint(f\"Number of features: {len(feature_cols)}\")\n\nX_train = train[feature_cols]\ny_train_reg = train['precipitation_mm']\ny_train_clf = train['electricity_shutdown'].astype(int)\nX_test = test[feature_cols]\n\n# Time-aware train/validation split\nprint(\"\\n=== Time-Aware Train/Validation Split ===\")\nsplit_idx = int(0.8 * len(X_train))\nX_train_split = X_train.iloc[:split_idx]\nX_val = X_train.iloc[split_idx:]\ny_reg_split = y_train_reg.iloc[:split_idx]\ny_reg_val = y_train_reg.iloc[split_idx:]\ny_clf_split = y_train_clf.iloc[:split_idx]\ny_clf_val = y_train_clf.iloc[split_idx:]\n\nprint(f\"Training split: {X_train_split.shape}\")\nprint(f\"Validation split: {X_val.shape}\")\n\n# Create preprocessing pipeline\nprint(\"\\n=== Creating Preprocessing Pipeline ===\")\npreprocessor = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Try different hyperparameters\nprint(\"\\n=== Hyperparameter Tuning ===\")\nbest_score = float('inf')\nbest_params = None\nbest_threshold = 0.5\n\nparam_combinations = [\n    {'max_iter': 100, 'learning_rate': 0.1, 'max_depth': None},\n    {'max_iter': 200, 'learning_rate': 0.05, 'max_depth': None},\n    {'max_iter': 300, 'learning_rate': 0.03, 'max_depth': None},\n    {'max_iter': 200, 'learning_rate': 0.1, 'max_depth': 8},\n    {'max_iter': 300, 'learning_rate': 0.05, 'max_depth': 10},\n]\n\nfor i, params in enumerate(param_combinations):\n    print(f\"Trying combination {i+1}/{len(param_combinations)}: {params}\")\n    \n    # Create model with current parameters\n    base_model = HistGradientBoostingRegressor(\n        random_state=42,\n        **params\n    )\n    \n    model = MultiOutputRegressor(base_model)\n    \n    # Create pipeline\n    pipeline = Pipeline([\n        ('preprocessor', preprocessor),\n        ('model', model)\n    ])\n    \n    # Train and predict\n    y_train_combined = np.column_stack([y_reg_split, y_clf_split])\n    pipeline.fit(X_train_split, y_train_combined)\n    \n    val_preds = pipeline.predict(X_val)\n    val_reg_pred = val_preds[:, 0]\n    val_clf_proba = np.clip(val_preds[:, 1], 0.0, 1.0)\n    \n    # Find best threshold\n    threshold, f1 = find_best_threshold(y_clf_val, val_clf_proba)\n    \n    # Calculate custom metric\n    score, f1, rmse = custom_metric(y_reg_val, val_reg_pred, y_clf_val, val_clf_proba, threshold)\n    \n    print(f\"  RMSE: {rmse:.4f}, F1: {f1:.4f}, Score: {score:.4f}, Threshold: {threshold:.3f}\")\n    \n    if score < best_score:\n        best_score = score\n        best_params = params\n        best_threshold = threshold\n        best_pipeline = pipeline\n\nprint(f\"\\nBest parameters: {best_params}\")\nprint(f\"Best threshold: {best_threshold:.3f}\")\nprint(f\"Best score: {best_score:.4f}\")\n\n# Train final model on full training data\nprint(\"\\n=== Training Final Model on Full Data ===\")\nfinal_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', MultiOutputRegressor(HistGradientBoostingRegressor(\n        random_state=42,\n        **best_params\n    )))\n])\n\ny_train_combined = np.column_stack([y_train_reg, y_train_clf])\nfinal_pipeline.fit(X_train, y_train_combined)\n\n# Make predictions on test set\nprint(\"\\n=== Making Test Predictions ===\")\ntest_preds = final_pipeline.predict(X_test)\ntest_reg_pred = test_preds[:, 0]\ntest_clf_proba = np.clip(test_preds[:, 1], 0.0, 1.0)\ntest_clf_pred = (test_clf_proba >= best_threshold).astype(int)\n\nprint(f\"Test predictions shape: {test_preds.shape}\")\nprint(f\"Precipitation range: {test_reg_pred.min():.3f} to {test_reg_pred.max():.3f}\")\nprint(f\"Electricity shutdown predictions: {np.bincount(test_clf_pred)}\")\n\n# Save predictions\nprint(\"\\n=== Saving Predictions ===\")\nsubmission = pd.read_csv(\"submission.csv\")\nsubmission['precipitation_mm'] = test_reg_pred\nsubmission['electricity_shutdown'] = test_clf_pred\nsubmission.to_csv(\"my_submission.csv\", index=False)\nprint(\"Saved predictions to my_submission.csv\")\n\nprint(\"\\n=== Summary ===\")\nprint(\"1. Added comprehensive time features including cyclical encoding\")\nprint(\"2. Created lag features for important weather variables\")\nprint(\"3. Used time-aware train/validation split\")\nprint(\"4. Performed hyperparameter tuning with custom metric\")\nprint(\"5. Found optimal threshold for binary classification\")\nprint(\"6. Trained final model on full data\")\nprint(\"7. Generated and saved predictions\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Step 6\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score, mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef custom_metric(y_true_reg, y_pred_reg, y_true_clf, y_pred_clf_proba, threshold=0.5):\n    \"\"\"Calculate custom metric: RMSE * (1 - F1)\"\"\"\n    y_pred_bin = (y_pred_clf_proba >= threshold).astype(int)\n    f1 = f1_score(y_true_clf, y_pred_bin, zero_division=0)\n    rmse = np.sqrt(np.mean((y_true_reg - y_pred_reg) ** 2))\n    return rmse * (1 - f1), f1, rmse\n\ndef find_best_threshold(y_true, y_proba):\n    \"\"\"Find optimal threshold for binary classification\"\"\"\n    best_thr, best_f1 = 0.5, -1.0\n    for thr in np.linspace(0.0, 1.0, 101):\n        y_pred = (y_proba >= thr).astype(int)\n        f1 = f1_score(y_true, y_pred, zero_division=0)\n        if f1 > best_f1:\n            best_f1, best_thr = f1, thr\n    return best_thr, best_f1\n\ndef add_time_features(df):\n    \"\"\"Add comprehensive time features\"\"\"\n    df = df.copy()\n    df['time'] = pd.to_datetime(df['time'])\n    \n    # Basic time features\n    df['hour'] = df['time'].dt.hour\n    df['dayofweek'] = df['time'].dt.dayofweek\n    df['month'] = df['time'].dt.month\n    df['day'] = df['time'].dt.day\n    df['year'] = df['time'].dt.year\n    \n    # Cyclical features (better for ML)\n    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n    \n    # Binary features\n    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n    df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n    \n    return df\n\ndef create_lag_features(df, cols, lags=[1, 3, 6]):\n    \"\"\"Create lag features for important columns\"\"\"\n    df = df.copy()\n    for col in cols:\n        for lag in lags:\n            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n    return df\n\ndef create_rolling_features(df, cols, windows=[3, 6]):\n    \"\"\"Create rolling window features\"\"\"\n    df = df.copy()\n    for col in cols:\n        for window in windows:\n            df[f'{col}_rolling_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n            df[f'{col}_rolling_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n    return df\n\n# Load and prepare data\nprint(\"=== Loading and Preparing Data ===\")\ntrain = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\n\n# Add time features\ntrain = add_time_features(train)\ntest = add_time_features(test)\n\n# Create lag and rolling features for important weather variables\nimportant_cols = ['temp_2m_C', 'rel_humidity_2m_pct', 'pressure_msl_hPa', 'wind_speed_10m_kph']\ntrain = create_lag_features(train, important_cols)\ntest = create_lag_features(test, important_cols)\n\ntrain = create_rolling_features(train, important_cols)\ntest = create_rolling_features(test, important_cols)\n\n# Add lag features for precipitation only in train set\ntrain = create_lag_features(train, ['precipitation_mm'])\ntrain = create_rolling_features(train, ['precipitation_mm'])\n\nprint(f\"Train shape after feature engineering: {train.shape}\")\nprint(f\"Test shape after feature engineering: {test.shape}\")\n\n# Prepare features and targets\n# Only use features that exist in both train and test sets\ntrain_cols = set(train.columns)\ntest_cols = set(test.columns)\ncommon_cols = train_cols.intersection(test_cols)\nexclude_cols = {'ID', 'time', 'precipitation_mm', 'electricity_shutdown'}\nfeature_cols = [col for col in common_cols if col not in exclude_cols]\nprint(f\"Number of features: {len(feature_cols)}\")\n\nX_train = train[feature_cols]\ny_train_reg = train['precipitation_mm']\ny_train_clf = train['electricity_shutdown'].astype(int)\nX_test = test[feature_cols]\n\n# Time-aware train/validation split\nprint(\"\\n=== Time-Aware Train/Validation Split ===\")\nsplit_idx = int(0.8 * len(X_train))\nX_train_split = X_train.iloc[:split_idx]\nX_val = X_train.iloc[split_idx:]\ny_reg_split = y_train_reg.iloc[:split_idx]\ny_reg_val = y_train_reg.iloc[split_idx:]\ny_clf_split = y_train_clf.iloc[:split_idx]\ny_clf_val = y_train_clf.iloc[split_idx:]\n\nprint(f\"Training split: {X_train_split.shape}\")\nprint(f\"Validation split: {X_val.shape}\")\nprint(f\"Class distribution in training: {np.bincount(y_clf_split)}\")\nprint(f\"Class distribution in validation: {np.bincount(y_clf_val)}\")\n\n# Create preprocessing pipeline\nprint(\"\\n=== Creating Preprocessing Pipeline ===\")\npreprocessor = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Try different hyperparameters with focus on class imbalance\nprint(\"\\n=== Hyperparameter Tuning ===\")\nbest_score = float('inf')\nbest_params = None\nbest_threshold = 0.5\n\nparam_combinations = [\n    {'max_iter': 500, 'learning_rate': 0.05, 'max_depth': None, 'min_samples_leaf': 20},\n    {'max_iter': 300, 'learning_rate': 0.1, 'max_depth': 8, 'min_samples_leaf': 10},\n    {'max_iter': 400, 'learning_rate': 0.03, 'max_depth': None, 'min_samples_leaf': 30},\n    {'max_iter': 600, 'learning_rate': 0.02, 'max_depth': 10, 'min_samples_leaf': 25},\n    {'max_iter': 200, 'learning_rate': 0.15, 'max_depth': 6, 'min_samples_leaf': 15},\n]\n\nfor i, params in enumerate(param_combinations):\n    print(f\"Trying combination {i+1}/{len(param_combinations)}: {params}\")\n    \n    # Create model with current parameters\n    base_model = HistGradientBoostingRegressor(\n        random_state=42,\n        **params\n    )\n    \n    model = MultiOutputRegressor(base_model)\n    \n    # Create pipeline\n    pipeline = Pipeline([\n        ('preprocessor', preprocessor),\n        ('model', model)\n    ])\n    \n    # Train and predict\n    y_train_combined = np.column_stack([y_reg_split, y_clf_split])\n    pipeline.fit(X_train_split, y_train_combined)\n    \n    val_preds = pipeline.predict(X_val)\n    val_reg_pred = val_preds[:, 0]\n    val_clf_proba = np.clip(val_preds[:, 1], 0.0, 1.0)\n    \n    # Find best threshold\n    threshold, f1 = find_best_threshold(y_clf_val, val_clf_proba)\n    \n    # Calculate custom metric\n    score, f1, rmse = custom_metric(y_reg_val, val_reg_pred, y_clf_val, val_clf_proba, threshold)\n    \n    print(f\"  RMSE: {rmse:.4f}, F1: {f1:.4f}, Score: {score:.4f}, Threshold: {threshold:.3f}\")\n    \n    if score < best_score:\n        best_score = score\n        best_params = params\n        best_threshold = threshold\n        best_pipeline = pipeline\n\nprint(f\"\\nBest parameters: {best_params}\")\nprint(f\"Best threshold: {best_threshold:.3f}\")\nprint(f\"Best score: {best_score:.4f}\")\n\n# Train final model on full training data\nprint(\"\\n=== Training Final Model on Full Data ===\")\nfinal_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', MultiOutputRegressor(HistGradientBoostingRegressor(\n        random_state=42,\n        **best_params\n    )))\n])\n\ny_train_combined = np.column_stack([y_train_reg, y_train_clf])\nfinal_pipeline.fit(X_train, y_train_combined)\n\n# Make predictions on test set\nprint(\"\\n=== Making Test Predictions ===\")\ntest_preds = final_pipeline.predict(X_test)\ntest_reg_pred = test_preds[:, 0]\ntest_clf_proba = np.clip(test_preds[:, 1], 0.0, 1.0)\ntest_clf_pred = (test_clf_proba >= best_threshold).astype(int)\n\nprint(f\"Test predictions shape: {test_preds.shape}\")\nprint(f\"Precipitation range: {test_reg_pred.min():.3f} to {test_reg_pred.max():.3f}\")\nprint(f\"Electricity shutdown predictions: {np.bincount(test_clf_pred)}\")\nprint(f\"Electricity shutdown probabilities range: {test_clf_proba.min():.3f} to {test_clf_proba.max():.3f}\")\n\n# Save predictions\nprint(\"\\n=== Saving Predictions ===\")\nsubmission = pd.read_csv(\"submission.csv\")\nsubmission['precipitation_mm'] = test_reg_pred\nsubmission['electricity_shutdown'] = test_clf_pred\nsubmission.to_csv(\"my_final_submission.csv\", index=False)\nprint(\"Saved predictions to my_final_submission.csv\")\n\n# Show some statistics\nprint(\"\\n=== Prediction Statistics ===\")\nprint(f\"Number of predicted electricity shutdowns: {test_clf_pred.sum()}\")\nprint(f\"Percentage of predicted shutdowns: {test_clf_pred.mean()*100:.2f}%\")\nprint(f\"Original training shutdown percentage: {y_train_clf.mean()*100:.2f}%\")\n\nprint(\"\\n=== Summary ===\")\nprint(\"1. Added comprehensive time features including cyclical encoding\")\nprint(\"2. Created lag and rolling features for important weather variables\")\nprint(\"3. Used time-aware train/validation split\")\nprint(\"4. Performed hyperparameter tuning with focus on class imbalance\")\nprint(\"5. Found optimal threshold for binary classification\")\nprint(\"6. Trained final model on full data\")\nprint(\"7. Generated and saved predictions\")\nprint(\"8. Handled class imbalance through threshold optimization\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Step 7\n\"\"\"\nComplete Multi-Task Learning Pipeline for Kaggle Competition\n\nThis script demonstrates the complete pipeline for predicting:\n1. Precipitation (regression)\n2. Electricity shutdown (binary classification)\n\nCustom metric: RMSE * (1 - F1)\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score, mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef custom_metric(y_true_reg, y_pred_reg, y_true_clf, y_pred_clf_proba, threshold=0.5):\n    \"\"\"Calculate custom metric: RMSE * (1 - F1)\"\"\"\n    y_pred_bin = (y_pred_clf_proba >= threshold).astype(int)\n    f1 = f1_score(y_true_clf, y_pred_bin, zero_division=0)\n    rmse = np.sqrt(np.mean((y_true_reg - y_pred_reg) ** 2))\n    return rmse * (1 - f1), f1, rmse\n\ndef find_best_threshold(y_true, y_proba):\n    \"\"\"Find optimal threshold for binary classification\"\"\"\n    best_thr, best_f1 = 0.5, -1.0\n    for thr in np.linspace(0.0, 1.0, 101):\n        y_pred = (y_proba >= thr).astype(int)\n        f1 = f1_score(y_true, y_pred, zero_division=0)\n        if f1 > best_f1:\n            best_f1, best_thr = f1, thr\n    return best_thr, best_f1\n\ndef add_time_features(df):\n    \"\"\"Add comprehensive time features\"\"\"\n    df = df.copy()\n    df['time'] = pd.to_datetime(df['time'])\n    \n    # Basic time features\n    df['hour'] = df['time'].dt.hour\n    df['dayofweek'] = df['time'].dt.dayofweek\n    df['month'] = df['time'].dt.month\n    df['day'] = df['time'].dt.day\n    df['year'] = df['time'].dt.year\n    \n    # Cyclical features (better for ML)\n    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n    \n    # Binary features\n    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n    df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n    \n    return df\n\ndef create_lag_features(df, cols, lags=[1, 3, 6]):\n    \"\"\"Create lag features for important columns\"\"\"\n    df = df.copy()\n    for col in cols:\n        for lag in lags:\n            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n    return df\n\ndef create_rolling_features(df, cols, windows=[3, 6]):\n    \"\"\"Create rolling window features\"\"\"\n    df = df.copy()\n    for col in cols:\n        for window in windows:\n            df[f'{col}_rolling_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n            df[f'{col}_rolling_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n    return df\n\ndef main():\n    print(\"=== Complete Multi-Task Learning Pipeline ===\")\n    \n    # Step 1: Load Data\n    print(\"\\n1. Loading Data...\")\n    train = pd.read_csv(\"train.csv\")\n    test = pd.read_csv(\"test.csv\")\n    submission = pd.read_csv(\"submission.csv\")\n    \n    print(f\"   Train shape: {train.shape}\")\n    print(f\"   Test shape: {test.shape}\")\n    \n    # Step 2: Feature Engineering\n    print(\"\\n2. Feature Engineering...\")\n    \n    # Add time features\n    train = add_time_features(train)\n    test = add_time_features(test)\n    \n    # Create lag and rolling features for important weather variables\n    important_cols = ['temp_2m_C', 'rel_humidity_2m_pct', 'pressure_msl_hPa', 'wind_speed_10m_kph']\n    train = create_lag_features(train, important_cols)\n    test = create_lag_features(test, important_cols)\n    \n    train = create_rolling_features(train, important_cols)\n    test = create_rolling_features(test, important_cols)\n    \n    # Add lag features for precipitation only in train set\n    train = create_lag_features(train, ['precipitation_mm'])\n    train = create_rolling_features(train, ['precipitation_mm'])\n    \n    print(f\"   Train features: {train.shape[1]}\")\n    print(f\"   Test features: {test.shape[1]}\")\n    \n    # Step 3: Prepare Features and Targets\n    print(\"\\n3. Preparing Features and Targets...\")\n    \n    # Only use features that exist in both train and test sets\n    train_cols = set(train.columns)\n    test_cols = set(test.columns)\n    common_cols = train_cols.intersection(test_cols)\n    exclude_cols = {'ID', 'time', 'precipitation_mm', 'electricity_shutdown'}\n    feature_cols = [col for col in common_cols if col not in exclude_cols]\n    \n    X_train = train[feature_cols]\n    y_train_reg = train['precipitation_mm']\n    y_train_clf = train['electricity_shutdown'].astype(int)\n    X_test = test[feature_cols]\n    \n    print(f\"   Number of features: {len(feature_cols)}\")\n    print(f\"   Class distribution: {np.bincount(y_train_clf)}\")\n    \n    # Step 4: Time-Aware Train/Validation Split\n    print(\"\\n4. Time-Aware Train/Validation Split...\")\n    split_idx = int(0.8 * len(X_train))\n    X_train_split = X_train.iloc[:split_idx]\n    X_val = X_train.iloc[split_idx:]\n    y_reg_split = y_train_reg.iloc[:split_idx]\n    y_reg_val = y_train_reg.iloc[split_idx:]\n    y_clf_split = y_train_clf.iloc[:split_idx]\n    y_clf_val = y_train_clf.iloc[split_idx:]\n    \n    print(f\"   Training split: {X_train_split.shape}\")\n    print(f\"   Validation split: {X_val.shape}\")\n    \n    # Step 5: Hyperparameter Tuning\n    print(\"\\n5. Hyperparameter Tuning...\")\n    \n    preprocessor = Pipeline([\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())\n    ])\n    \n    best_score = float('inf')\n    best_params = None\n    best_threshold = 0.5\n    \n    param_combinations = [\n        {'max_iter': 500, 'learning_rate': 0.05, 'max_depth': None, 'min_samples_leaf': 20},\n        {'max_iter': 300, 'learning_rate': 0.1, 'max_depth': 8, 'min_samples_leaf': 10},\n        {'max_iter': 400, 'learning_rate': 0.03, 'max_depth': None, 'min_samples_leaf': 30},\n        {'max_iter': 600, 'learning_rate': 0.02, 'max_depth': 10, 'min_samples_leaf': 25},\n        {'max_iter': 200, 'learning_rate': 0.15, 'max_depth': 6, 'min_samples_leaf': 15},\n    ]\n    \n    for i, params in enumerate(param_combinations):\n        print(f\"   Trying combination {i+1}/{len(param_combinations)}...\")\n        \n        base_model = HistGradientBoostingRegressor(random_state=42, **params)\n        model = MultiOutputRegressor(base_model)\n        \n        pipeline = Pipeline([\n            ('preprocessor', preprocessor),\n            ('model', model)\n        ])\n        \n        y_train_combined = np.column_stack([y_reg_split, y_clf_split])\n        pipeline.fit(X_train_split, y_train_combined)\n        \n        val_preds = pipeline.predict(X_val)\n        val_reg_pred = val_preds[:, 0]\n        val_clf_proba = np.clip(val_preds[:, 1], 0.0, 1.0)\n        \n        threshold, f1 = find_best_threshold(y_clf_val, val_clf_proba)\n        score, f1, rmse = custom_metric(y_reg_val, val_reg_pred, y_clf_val, val_clf_proba, threshold)\n        \n        print(f\"     RMSE: {rmse:.4f}, F1: {f1:.4f}, Score: {score:.4f}, Threshold: {threshold:.3f}\")\n        \n        if score < best_score:\n            best_score = score\n            best_params = params\n            best_threshold = threshold\n    \n    print(f\"\\n   Best parameters: {best_params}\")\n    print(f\"   Best threshold: {best_threshold:.3f}\")\n    print(f\"   Best score: {best_score:.4f}\")\n    \n    # Step 6: Train Final Model\n    print(\"\\n6. Training Final Model...\")\n    \n    final_pipeline = Pipeline([\n        ('preprocessor', preprocessor),\n        ('model', MultiOutputRegressor(HistGradientBoostingRegressor(\n            random_state=42,\n            **best_params\n        )))\n    ])\n    \n    y_train_combined = np.column_stack([y_train_reg, y_train_clf])\n    final_pipeline.fit(X_train, y_train_combined)\n    \n    # Step 7: Make Predictions\n    print(\"\\n7. Making Predictions...\")\n    \n    test_preds = final_pipeline.predict(X_test)\n    test_reg_pred = test_preds[:, 0]\n    test_clf_proba = np.clip(test_preds[:, 1], 0.0, 1.0)\n    test_clf_pred = (test_clf_proba >= best_threshold).astype(int)\n    \n    print(f\"   Test predictions shape: {test_preds.shape}\")\n    print(f\"   Precipitation range: {test_reg_pred.min():.3f} to {test_reg_pred.max():.3f}\")\n    print(f\"   Electricity shutdown predictions: {np.bincount(test_clf_pred)}\")\n    \n    # Step 8: Save Results\n    print(\"\\n8. Saving Results...\")\n    \n    submission['precipitation_mm'] = test_reg_pred\n    submission['electricity_shutdown'] = test_clf_pred\n    submission.to_csv(\"final_submission.csv\", index=False)\n    \n    print(\"   Saved predictions to final_submission.csv\")\n    \n    # Step 9: Summary Statistics\n    print(\"\\n9. Summary Statistics...\")\n    print(f\"   Number of predicted electricity shutdowns: {test_clf_pred.sum()}\")\n    print(f\"   Percentage of predicted shutdowns: {test_clf_pred.mean()*100:.2f}%\")\n    print(f\"   Original training shutdown percentage: {y_train_clf.mean()*100:.2f}%\")\n    \n    print(\"\\n=== Pipeline Complete! ===\")\n    print(\"Key Learnings:\")\n    print(\"1. Time series data requires time-aware splitting\")\n    print(\"2. Cyclical encoding of time features improves performance\")\n    print(\"3. Lag and rolling features capture temporal patterns\")\n    print(\"4. Class imbalance requires threshold optimization\")\n    print(\"5. Multi-output models can handle both tasks simultaneously\")\n    print(\"6. Custom metrics guide hyperparameter selection\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}